{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out embedding + clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "import os\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "robo_workspace = \"cyclinghelper\"\n",
    "robo_project = \"pro-cyclist-teams\"\n",
    "robo_version = 1\n",
    "robo_api_key = os.getenv(\"ROBOFLOW_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Exporting format folder in progress : 85.0%\n",
      "Version export complete for folder format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(27774) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Downloading Dataset Version Zip in pro-cyclist-teams-1 to folder:: 100%|██████████| 589/589 [00:00<00:00, 1885.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to pro-cyclist-teams-1 in folder:: 100%|██████████| 174/174 [00:00<00:00, 6075.77it/s]\n"
     ]
    }
   ],
   "source": [
    "rf = Roboflow(api_key=robo_api_key)\n",
    "project = rf.workspace(robo_workspace).project(robo_project)\n",
    "version = project.version(robo_version)\n",
    "dataset = version.download(\"folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Embeddings and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Adapted from this notebook - https://colab.research.google.com/drive/1EJFpca6IG8dPCZ2-WwEX5GTDetp1Pe7f?usp=sharing#scrollTo=0OwO0H8UMIA7\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import base64\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import umap\n",
    "import time\n",
    "\n",
    "model_name='ViT-B/32'\n",
    "\n",
    "# MPS for apple - switch to CUDA later if I use a GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "# device='cpu'\n",
    "\n",
    "openai_clip_model, openai__preprocess = clip.load(model_name,device)\n",
    "\n",
    "def get_openai_clip_embedding(imgs):\n",
    "  with torch.no_grad():\n",
    "    preprocessed = torch.stack([openai__preprocess(i) for i in imgs]).to(device)\n",
    "    features = openai_clip_model.encode_image(preprocessed)\n",
    "    features /= features.norm(dim=-1, keepdim=True)\n",
    "    return features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = get_openai_clip_embedding([Image.open('crops/Gent Wevelgem 2024_image_3.jpg')])\n",
    "e.cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and align with labels and images\n",
    "def image_to_data_uri(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    return \"data:image/jpeg;base64,\" + encoded_image\n",
    "\n",
    "SOURCE_DIR = 'pro-cyclist-teams-1/test/'\n",
    "\n",
    "labels = []\n",
    "openclip_embeddings = []\n",
    "openai_clip_embeddings = []\n",
    "train = []\n",
    "images = []\n",
    "image_paths = []\n",
    "\n",
    "class_ids = sorted(os.listdir(SOURCE_DIR))\n",
    "\n",
    "for class_id in class_ids:\n",
    "    source_subdir = os.path.join(SOURCE_DIR, class_id)\n",
    "    for image_path in glob.glob(source_subdir+'/*.jpg'):\n",
    "      try:\n",
    "        image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "        img = Image.open(image_path)\n",
    "        openai_clip_emb = get_openai_clip_embedding([img])\n",
    "        openai_clip_embeddings.append(openai_clip_emb.cpu().numpy())\n",
    "        labels.append(class_id)\n",
    "        images.append(image)\n",
    "        image_paths.append(str(image_path))\n",
    "      except:\n",
    "        print(f\"can't process {image_path}\")\n",
    "        pass\n",
    "\n",
    "# class associated with image\n",
    "labels = np.array(labels)\n",
    "# features extracted from image\n",
    "openai_clip_embeddings = np.array(openai_clip_embeddings)\n",
    "\n",
    "\n",
    "# local image path\n",
    "image_paths = np.array(image_paths)\n",
    "# cached images\n",
    "image_data_uris = {path: image_to_data_uri(path) for path in image_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def display_projections(\n",
    "    labels: np.ndarray,\n",
    "    projections: np.ndarray,\n",
    "    image_paths: np.ndarray,\n",
    "    image_data_uris: Dict[str, str],\n",
    "    show_legend: bool = False,\n",
    "    show_markers_with_text: bool = True\n",
    ") -> None:\n",
    "    # Create a separate trace for each unique label\n",
    "    unique_labels = np.unique(labels)\n",
    "    traces = []\n",
    "    for unique_label in unique_labels:\n",
    "        mask = labels == unique_label\n",
    "        customdata_masked = image_paths[mask]\n",
    "        trace = go.Scatter3d(\n",
    "            x=projections[mask][:, 0],\n",
    "            y=projections[mask][:, 1],\n",
    "            z=projections[mask][:, 2],\n",
    "            mode='markers+text' if show_markers_with_text else 'markers',\n",
    "            text=labels[mask],\n",
    "            customdata=customdata_masked,\n",
    "            name=str(unique_label),\n",
    "            marker=dict(size=8),\n",
    "            hovertemplate=\"<b>class: %{text}</b><br>path: %{customdata}<extra></extra>\"\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    # Create the 3D scatter plot\n",
    "    fig = go.Figure(data=traces)\n",
    "    fig.update_layout(\n",
    "        scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "        showlegend=show_legend\n",
    "    )\n",
    "\n",
    "    # Convert the chart to an HTML div string and add an ID to the div\n",
    "    plotly_div = fig.to_html(full_html=False, include_plotlyjs=False, div_id=\"scatter-plot-3d\")\n",
    "\n",
    "    # Define your JavaScript code for copying text on point click\n",
    "    javascript_code = f\"\"\"\n",
    "    <script>\n",
    "        function displayImage(imagePath) {{\n",
    "            var imageElement = document.getElementById('image-display');\n",
    "            var placeholderText = document.getElementById('placeholder-text');\n",
    "            var imageDataURIs = {image_data_uris};\n",
    "            imageElement.src = imageDataURIs[imagePath];\n",
    "            imageElement.style.display = 'block';\n",
    "            placeholderText.style.display = 'none';\n",
    "        }}\n",
    "\n",
    "        // Get the Plotly chart element by its ID\n",
    "        var chartElement = document.getElementById('scatter-plot-3d');\n",
    "\n",
    "        // Add a click event listener to the chart element\n",
    "        chartElement.on('plotly_click', function(data) {{\n",
    "            var customdata = data.points[0].customdata;\n",
    "            displayImage(customdata);\n",
    "        }});\n",
    "    </script>\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an HTML template including the chart div and JavaScript code\n",
    "    html_template = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "            <style>\n",
    "                #image-container {{\n",
    "                    position: fixed;\n",
    "                    top: 0;\n",
    "                    left: 0;\n",
    "                    width: 200px;\n",
    "                    height: 200px;\n",
    "                    padding: 5px;\n",
    "                    border: 1px solid #ccc;\n",
    "                    background-color: white;\n",
    "                    z-index: 1000;\n",
    "                    box-sizing: border-box;\n",
    "                    display: flex;\n",
    "                    align-items: center;\n",
    "                    justify-content: center;\n",
    "                    text-align: center;\n",
    "                }}\n",
    "                #image-display {{\n",
    "                    width: 100%;\n",
    "                    height: 100%;\n",
    "                    object-fit: contain;\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            {plotly_div}\n",
    "            <div id=\"image-container\">\n",
    "                <img id=\"image-display\" src=\"\" alt=\"Selected image\" style=\"display: none;\" />\n",
    "                <p id=\"placeholder-text\">Click on a data entry to display an image</p>\n",
    "            </div>\n",
    "            {javascript_code}\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Display the HTML template in the Jupyter Notebook\n",
    "    return(html_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_clip_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating projections with UMAP took: 1.16 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "projections = umap.UMAP(n_components=3).fit_transform(openai_clip_embeddings)\n",
    "end = time.time()\n",
    "print(f\"generating projections with UMAP took: {(end-start):.2f} sec\")\n",
    "html_template = display_projections(\n",
    "    labels=labels,\n",
    "    projections=projections,\n",
    "    image_paths=image_paths,\n",
    "    image_data_uris=image_data_uris\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML template saved as test_template.html\n"
     ]
    }
   ],
   "source": [
    "# Specify the filename\n",
    "filename = 'test_template.html'\n",
    "\n",
    "# Open the file in write mode and save the HTML template\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(html_template)\n",
    "\n",
    "print(f\"HTML template saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import YOLO\n",
    "# Test out the trained model on video snippets\n",
    "model = 'models/best.pt'\n",
    "video = 'video_snippets/Gent Wevelgem 2024_snippet_2.mp4'\n",
    "\n",
    "model = YOLO(model)\n",
    "model.predict(video, save=True,conf=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
